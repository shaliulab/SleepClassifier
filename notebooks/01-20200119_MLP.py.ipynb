{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP) for binary classification\n",
    "\n",
    "1. A multilayer perceptron is a stack of fully connected (FC) layers\n",
    "2. Binary classification indicates the output can be treated as a coin toss(Yes/No or ClassA/ClassB)\n",
    "\n",
    "This notebook provides hanlders to generate MLP models with varying number of hidden layers and nodes per layer using tensorflow.keras\n",
    "\n",
    "* Tensorflow is the software module, availabe here as a Python module) that provides the algorithms, data structures, etc necessary to do ML.\n",
    "* It is not the only one, but it is one of the few with great support, development and compatibility with GPU/TPU. This means the same code can be executed on a ordinary CPU, but also much faster chips like a GPU.\n",
    "* Another very good software module is Torch (and its Python module, pytorch). Tensorflow is made by Google and Torch by Facebook.\n",
    "* Keras runs on top of tensorflow to abstract away the fiddly details of the ML and provide default and reasonable settings for most things. It just simplifies the Tensorflow API (the Tensorflow commands)\n",
    "* Keras is to Tensorflow what simple English is to Shakespeare English (with the nuance that one can still implement advanced/complex models without the need to write actual Tensorflow code)\n",
    "\n",
    "The notebook is compatible with Snakemake. See Snakemake cell below if you wanna know more or disable Snakemake.\n",
    "You probably want to disable Snakemake if you want to provide the loomfile, etc manually just by hardcoding them in the notebook.\n",
    "\n",
    "Workflow and details\n",
    "===========================\n",
    "\n",
    "1. Load software dependencies (Python modules)\n",
    "\n",
    "\n",
    "2. Load data to Python as an AnnData object (from a loomfile)\n",
    "\n",
    " 2.1 The data can be selected via Snakemake or via hardcoded paths/variables\n",
    "\n",
    "3. **Define a model** -> Specify architecture i.e. how many layers, how many nodes, activation functions, etc\n",
    "4. **Compile a model** -> Provide a loss function, an optimizer and optionally performance metrics\n",
    "  * The loss function is the mathematical expression that evaluates the error i.e. the difference between truth and prediction.\n",
    "  The loss function depends on the prediction task. It's not the same if the model is performing regression, classification, etc. And even within classification (like this case) several losses could be used\n",
    "  \n",
    "  * The optimizer is the algorithm that given a loss (i.e. the output of the loss _function_) updates the weights (parameters) of the model in order to decrease said loss in the next training iteration. This can be done in different ways,\n",
    "  but most algorithms consist of a variation of Stochastic Gradient Descent (SGD) and the standard backpropagation algorithm.\n",
    "  \n",
    "  * A performance metric quantifies in some way how well the model is doing. The loss is a performance metric itself, but it's hard to interpret in absolut terms.\n",
    "  For a classification task, the accuracy is one possible metric to look at (just % predictions are correct).\n",
    "    \n",
    "\n",
    "5. **Train a model** -> Specify more details about how the training should happen (batch size, number of epochs, ...) and actually expose the model to the data and make it \"learn\"\n",
    "\n",
    "  * Batch size is the number of datapoints (fly brain cells in this case) that are passed through the network before the loss is updated. Each new value of the loss is computed on `batch_size` number of cells.\n",
    "  A default and common batch size is 32.\n",
    "  One could use the whole dataset as batch, but that would take painfully long (it would have to give a prediction for ALL cells and then compute the loss) and the obtained loss is probably gonna be very similar to the loss obtained on a small smaple (batch). One could also use a batch of 1, which is as fast as it can get, but then it can be very noisy. 32 is a nice value because it's a nice compromise between robust and efficient loss computations.\n",
    "  \n",
    "  * One epoch is fullfilled when the model has seen each datapoint (brain cell) once. This can happen in any number of iterations. If the batch size is equal to the size of the dataset, one iteration is one epoch. If the batch size is 1, then a number of iterations equal to the size of the dataset is needed for an epoch to be completed. If the batch size is 32, then we need `(dataset_size / 32 + 1)` iterations.\n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Tuple, Union\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibflysleep/anaconda3/envs/TF/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "import anndata\n",
    "import scanpy\n",
    "import loompy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection # for train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sleepapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(\n",
    "    2021\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the software versions, for reproducibility\n",
    "\n",
    "* `Python`\n",
    "* `tensorflow`\n",
    "* `scanpy`\n",
    "* `loompy`\n",
    "* `anndata`\n",
    "\n",
    "are highly susceptible to change their behavior significantly, thus this notebook is actually unlikely to work unless your versions match exactly to those listed below\n",
    "\n",
    "If it works, even with different versions, it either is a miracle or you dont get an error but the results are wrong... \n",
    "\n",
    "The right version of this packages is available on cv1 in the `TF` conda environment / kernel\n",
    "You can select this environment by going to the top right corner and selecting the kernel called `TF`\n",
    "\n",
    "Please note. Tensorflow has system dependencies to work with the GPU. More concretely it needs the right CUDA, cuDNN and friends. So checking the tensorflow version is not enough if you want to run the GPU.\n",
    "Make sure the right CUDA, cuDNN, etc (right meaning compatible with tensorflow 2.1.0) is available in your system. It is on cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy    : 1.6.0\n",
      "numpy     : 1.18.1\n",
      "sys       : 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "anndata   : 0.7.5\n",
      "loompy    : 3.0.6\n",
      "matplotlib: 3.1.3\n",
      "tensorflow: 2.1.0\n",
      "h5py      : 2.10.0\n",
      "logging   : 0.5.1.2\n",
      "pandas    : 1.2.0\n",
      "sklearn   : 0.22.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardcoded\n",
    "\n",
    "If you are a human, run this to specify the paths/variables below\n",
    "AND DONT RUN THE CELL STARTING WITH TRY: SNAKEMAKE. Alternatively, run the Snakamek cell if you want to load the default values\n",
    "\n",
    "If you are a machine, this cell is irrelevant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loomfile = \"/1TB/Cloud/Lab/Projects/SleepSignature/workflow/results/20201224/scran/y_KCs/y_KCs_grouping-Condition.loom\" # path to the loomfile with the data\n",
    "# NOTE: even if it has scran on the name, it is not scran normalized. You can check because the counts are all integers\n",
    "# The correction factor is there through as a cell metadata column called sizeFactor. But it is not applied to the data when saving to the loomfile\n",
    "# One can optionally apply it after loading the loomfile. But we dont for now\n",
    "\n",
    "# column in the cell metadata that we want to predict\n",
    "# one could try to predict Run (batch), Genotype (DGRP line) or Condition (detailed experimental condition) too\n",
    "target = \"Treatment\"  \n",
    "\n",
    "# where to save the weights of the model\n",
    "# the model gets saved as two files inside the directory specified in the path below\n",
    "# a json file with the structure\n",
    "# an h5fd file with the weights\n",
    "model_name = \"/1TB/Cloud/Lab/Projects/SleepSignature/sleep_classifier/results/y_KCs_Treatment\"\n",
    "\n",
    "# where to save the results of the model\n",
    "summary_csv = '/1TB/Cloud/Lab/Projects/SleepSignature/sleep_classifier/results/y_KCs_Treatment.csv'\n",
    "test_str_txt = '/1TB/Cloud/Lab/Projects/SleepSignature/sleep_classifier/results/y_KCs_Treatment.txt'\n",
    "\n",
    "random_labels = False # whether we should make up the labels (random_labels=True) or stick to the real ones (False)\n",
    "# it would make sense to set to True if one wants to check what would be the performance on a completely random i.e. unpredictable target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snakemake\n",
    "\n",
    "If you are a human run this to load the default Snakemake values.\n",
    "\n",
    "If you are a machine, the result of this cell will be different becuase you will run this notebook as many times as cell types available, and on each run you pass a different loomfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    snakemake\n",
    "    up = \"../..\"\n",
    "\n",
    "\n",
    "except NameError:\n",
    "    import json\n",
    "    class AttrDict(dict):\n",
    "        __getattr__ = dict.__getitem__\n",
    "        __setattr__ = dict.__setitem__\n",
    "\n",
    "    with open(\"01-MLP.json\", 'r') as fh:\n",
    "        snakemake = AttrDict(json.load(fh))\n",
    "        up = \"../\"\n",
    "        \n",
    "loomfile = snakemake.input[\"loomfile\"]\n",
    "model_name = snakemake.params[\"model\"]\n",
    "summary_csv = snakemake.output[\"summary_csv\"]\n",
    "test_str_txt = snakemake.output[\"test_str_txt\"]\n",
    "target = snakemake.params[\"target\"]\n",
    "random_labels = False#snakemake.params[\"random_labels\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the loomfile and target are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/1TB/Cloud/Lab/Projects/SleepSignature/workflow/results/20201224/scran/y_KCs/y_KCs_grouping-Condition.loom\n",
      "Treatment\n",
      "/1TB/Cloud/Lab/Projects/SleepSignature/sleep_classifier/results/y_KCs_Treatment\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(loomfile)\n",
    "print(target)\n",
    "print(model_name)\n",
    "print(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually load the selected loomfile into Python as an anndata object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /1TB/Cloud/Lab/Projects/SleepSignature/workflow/results/20201224/scran/y_KCs/y_KCs_grouping-Condition.loom\n",
      "Transposing loom dataset so rows become cells and columns become genes\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibflysleep/anaconda3/envs/TF/lib/python3.7/site-packages/anndata/_core/anndata.py:119: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "adata = sleepapp.loom2anndata(loomfile) # provides an anndata.AnnData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CellID</th>\n",
       "      <th>ClusterID</th>\n",
       "      <th>Clusterings.0</th>\n",
       "      <th>Clusterings.1</th>\n",
       "      <th>Clusterings.2</th>\n",
       "      <th>Clusterings.3</th>\n",
       "      <th>Clusterings.4</th>\n",
       "      <th>Clusterings.5</th>\n",
       "      <th>Clusterings.6</th>\n",
       "      <th>...</th>\n",
       "      <th>Sleep_Stage</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>detected</th>\n",
       "      <th>discard</th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>propZero</th>\n",
       "      <th>sizeFactor</th>\n",
       "      <th>sum</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6-9 days</td>\n",
       "      <td>AAACCCAGTTAGGGAC-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1298</td>\n",
       "      <td>2802.0</td>\n",
       "      <td>0.870135</td>\n",
       "      <td>0.483911</td>\n",
       "      <td>2802.0</td>\n",
       "      <td>2802.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6-9 days</td>\n",
       "      <td>AAACCCATCCTACTGC-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1664</td>\n",
       "      <td>4794.0</td>\n",
       "      <td>0.833517</td>\n",
       "      <td>0.765969</td>\n",
       "      <td>4794.0</td>\n",
       "      <td>4794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6-9 days</td>\n",
       "      <td>AAAGGATTCCTACGGG-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2362</td>\n",
       "      <td>8026.0</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>1.449857</td>\n",
       "      <td>8026.0</td>\n",
       "      <td>8026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6-9 days</td>\n",
       "      <td>AAAGGGCGTTTGAAAG-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1600</td>\n",
       "      <td>4022.0</td>\n",
       "      <td>0.839920</td>\n",
       "      <td>0.683532</td>\n",
       "      <td>4022.0</td>\n",
       "      <td>4022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6-7 days</td>\n",
       "      <td>AAAGGGCTCCTACCGT-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 20</td>\n",
       "      <td>Wake</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1625</td>\n",
       "      <td>4303.0</td>\n",
       "      <td>0.837419</td>\n",
       "      <td>0.700299</td>\n",
       "      <td>4303.0</td>\n",
       "      <td>4303.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age                                             CellID ClusterID  \\\n",
       "0  6-9 days  AAACCCAGTTAGGGAC-fc9729__DGRP_Mix_Sleep_Depriv...         4   \n",
       "1  6-9 days  AAACCCATCCTACTGC-fc9729__DGRP_Mix_Sleep_Depriv...         4   \n",
       "2  6-9 days  AAAGGATTCCTACGGG-fc9729__DGRP_Mix_Sleep_Depriv...         4   \n",
       "3  6-9 days  AAAGGGCGTTTGAAAG-fc9729__DGRP_Mix_Sleep_Depriv...         4   \n",
       "4  6-7 days  AAAGGGCTCCTACCGT-fc9729__DGRP_Mix_Sleep_Depriv...         4   \n",
       "\n",
       "   Clusterings.0  Clusterings.1  Clusterings.2  Clusterings.3  Clusterings.4  \\\n",
       "0              3              4              3              4              4   \n",
       "1              3              4              3              4              4   \n",
       "2              3              4              3              4              4   \n",
       "3              3              4              3              4              4   \n",
       "4              3              4              3              4              4   \n",
       "\n",
       "   Clusterings.5  Clusterings.6  ...  Sleep_Stage  Treatment detected  \\\n",
       "0              2              0  ...         ZT 8       Wake   1298.0   \n",
       "1              2              0  ...         ZT 8       Wake   1664.0   \n",
       "2              2              0  ...         ZT 8       Wake   2362.0   \n",
       "3              2              0  ...         ZT 8       Wake   1600.0   \n",
       "4              2              0  ...        ZT 20       Wake   1625.0   \n",
       "\n",
       "   discard  nGene    nUMI  propZero  sizeFactor     sum   total  \n",
       "0        0   1298  2802.0  0.870135    0.483911  2802.0  2802.0  \n",
       "1        0   1664  4794.0  0.833517    0.765969  4794.0  4794.0  \n",
       "2        0   2362  8026.0  0.763682    1.449857  8026.0  8026.0  \n",
       "3        0   1600  4022.0  0.839920    0.683532  4022.0  4022.0  \n",
       "4        0   1625  4303.0  0.837419    0.700299  4303.0  4303.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two classes:\n",
    "\n",
    "    * One for the dataset\n",
    "    * One for the model\n",
    "\n",
    "This is not needed, but it can be useful if one ends up repeating the same things over and over again, to keep DRY (Dont Repeat Yourself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLModel class has the following advantages:\n",
    "\n",
    "* it knows how many classes there are on the dataset.\n",
    "* it selects the right loss based on the number of classes.\n",
    "* it encodes the labels to digits automatically for you (this is needed by the model, since it's much easier to preidct 1/0 over Wake/Sleep).\n",
    "* it adds extra defaults on top of those from Keras, minimising the code you need to write to define an MLP (MultiLayer Perceptron). This is done with the sequential_model method.\n",
    "* it automates saving / loading the model to just running the .save and .load methos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel:\n",
    "    \"\"\"\n",
    "    Provide keras modelling\n",
    "    Assume the inheriting class provides:\n",
    "    \n",
    "    * self.get_labels -> method returning list of labels\n",
    "    * self.n_features -> attribute yielding number of features (genes)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "        self._le = LabelEncoder()\n",
    "        \n",
    "    \n",
    "    def label_encoder(self, labels):\n",
    "        \"\"\"\n",
    "        Encode str labels to integers\n",
    "        \"\"\"\n",
    "        self._le = self._le.fit(labels)\n",
    "        y = self._le.transform(labels)\n",
    "        return y\n",
    "    \n",
    "    def get_encoding(self, codes):\n",
    "        labels = self._le.inverse_transform(codes)\n",
    "        mapping = {}\n",
    "    \n",
    "        for i in range(len(codes)):\n",
    "                print(f\"Label {labels[i]} = Code {codes[i]}\")\n",
    "                mapping[codes[i]] = labels[i]                \n",
    "        return mapping\n",
    "    \n",
    "    def get_last_layer_activation_function(self):\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            return \"sigmoid\"\n",
    "        else:\n",
    "            return \"softmax\"\n",
    "        \n",
    "        \n",
    "    def get_loss(self):\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            return \"binary_crossentropy\"\n",
    "        else:\n",
    "            return \"sparse_categorical_crossentropy\"\n",
    "  \n",
    "\n",
    "    def sequential_model(self, units: Union[Tuple, List] = (1000, 100)):\n",
    "        \"\"\"\n",
    "        Define a MLP model using the Keras Sequential API\n",
    "        \n",
    "        Arguments:\n",
    "            * units: Tuple or List where each ith element represents the number of units in the ith layer.\n",
    "            First number goes to first hidden layer i.e. input layer does not count.\n",
    "            Number of hidden units should decrease as we approach the last layer.\n",
    "        \n",
    "        Details:\n",
    "        \n",
    "        * Activation functions (AF):  An activation function is in charge of introducing non-linearity.\n",
    "        Intuitively, the data space can because of non linearities, be reshaped like plasticine, and not just like a towel.\n",
    "        We use always ReLU (Rectified Linear Unit) activation functions for all hidden layers because it is the most well-behaved AF.\n",
    "        \n",
    "        The AF in the output layer is different though and depends on the number of classes.        \n",
    "        \n",
    "        * Kernel intitializer: The weights need to get an initial value (a priori it could be all zero but that would actually make the learning not work).\n",
    "        The kernel initializer gives a random set of weights that will provide a good starting point (at least not a lethal one).\n",
    "        \n",
    "        * Fully Connected Layers: A fully connected layer gives each of its nodes a separate weight for each of the nodes on the previous layer\n",
    "        Such a layer is available in Keras as Dense.\n",
    "        \n",
    "        * The math behind this: Each node is doing the following computation\n",
    "        \n",
    "        f1 * w1 + f2 * w2 + ... + fN * wN\n",
    "        \n",
    "        where\n",
    "        * N is the number of nodes in the previous layer\n",
    "        * f1, f2, ..., f3 **is the same** for all nodes and it is the output of the previous layer\n",
    "        * w1, w2, ..., wN **is different** for each node (i.e each node has its own set of weights so its processes the data in a different way)        \n",
    "        \"\"\"\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        \n",
    "        \n",
    "        # add the first hidden layer\n",
    "        # it is special because its input shape is given by the number of features in the data\n",
    "        # and not the number of nodes in the previous hidden layer, obviously because there is no previous hidden layer\n",
    "        model.add(Dense(units[0], activation='relu', kernel_initializer='he_normal', input_shape=(self.n_features,), name=\"hidden_1\"))\n",
    "\n",
    "        # add a new hidden layer for every value in units\n",
    "        # the size of the hidden layer is the corresponding value\n",
    "        for i in range(1, len(units)):\n",
    "            model.add(Dense(units[i], activation='relu', kernel_initializer='he_normal', name=f\"hidden_{i+1}\"))\n",
    "\n",
    "        # add the output layer\n",
    "        model.add(Dense(self.n_classes, activation=self.get_last_layer_activation_function(), name=\"output\"))\n",
    "        self._model = model\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, x):\n",
    "        print(\"Setting model\")\n",
    "        self._model = x\n",
    "\n",
    "    def set_target(self, column_name):\n",
    "        self._target = column_name\n",
    "    \n",
    "    def compile(self, *args, **kwargs):\n",
    "        # compile the model\n",
    "        optimizer = kwargs.pop(\"optimizer\", \"adam\")\n",
    "        loss = kwargs.pop(\"loss\", \"sparse_categorical_crossentropy\")\n",
    "        metrics = kwargs.pop(\"metrics\", ['accuracy'])\n",
    "        self.model.compile(*args, optimizer=optimizer, loss=loss, metrics=metrics, **kwargs)\n",
    "        \n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        # fit the model\n",
    "        self.model.fit(X, y, *args, batch_size=32, verbose=3, **kwargs)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        yhat = self.model.predict(data)\n",
    "        \n",
    "        if self.n_classes == 1:\n",
    "            yhat = np.array([1-yhat, yhat])          \n",
    "        \n",
    "        pred_class = np.argmax(yhat)\n",
    "        class_score = np.max(yhat)\n",
    "        return {pred_class: class_score}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def save(self, model=\"model\"):\n",
    "        \n",
    "        if self.model is None:\n",
    "            return\n",
    "        else:\n",
    "            # serialize model to JSON\n",
    "            model_json = self.model.to_json()\n",
    "            with open(f\"{model}.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            self.model.save_weights(f\"{model}.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            \n",
    "    def load(self, model=\"model\"):\n",
    "        # later...\n",
    "        # load json and create model\n",
    "        output_model_json = \"\"\n",
    "        json_file = open(f\"{model}.json\", 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        print(loaded_model_json)\n",
    "        loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(f\"{model}.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        self.model = loaded_model\n",
    "        self.compile()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(MLModel):\n",
    "    \n",
    "    def __init__(self, adata, *args, **kwargs):\n",
    "        self._adata = adata\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def xtype(x):\n",
    "        return type(x).__module__\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        X = self._adata.X\n",
    "        #print(self.xtype(X))\n",
    "        if self.xtype(X) == \"numpy\":\n",
    "            print(\"Returning numpy\")\n",
    "            return X\n",
    "        else:\n",
    "            return X.toarray()\n",
    "\n",
    "    def get_features(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Return model imput features (genes) as a table of shape\n",
    "        cells x genes\n",
    "        \"\"\"\n",
    "        return self.X\n",
    "        \n",
    "    def get_labels(self, encode: bool = True, random: bool = False) -> List:\n",
    "        \"\"\"\n",
    "        Return learnable feature (label)\n",
    "        This should be a column in the obs table\n",
    "        that the model is postulated to be able to infer from the transcriptome\n",
    "        \n",
    "        Arguments:\n",
    "            * encode: the labels are returned encoded, i.e. each label becomes an integer starting from 0\n",
    "            * random: if True, a set of random labels is generated with number of classes equal to those in self._target\n",
    "        \"\"\"\n",
    "\n",
    "        if self._target is None:\n",
    "            print(\"Please set _target to a column name in the cell metadata (self._adata.obs)\")\n",
    "            return None\n",
    "\n",
    "        labels = self._adata.obs[self._target].values.tolist()\n",
    "\n",
    "        if encode:\n",
    "                labels = self.label_encoder(labels)\n",
    "        if random:\n",
    "            labels = np.random.randint(0, len(np.unique(labels)), self._adata.obs.shape[0])\n",
    "        \n",
    "        return labels\n",
    "            \n",
    "    \n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self._adata.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return len(self._adata.obs[self._target].unique())\n",
    "            \n",
    "\n",
    "    def train_test_split(self, test_size=0.33, *args, **kwargs):\n",
    "        X = self.get_features()\n",
    "        y = self.get_labels(*args, **kwargs)\n",
    "        indices = self.obs[\"CellID\"]\n",
    "        X_train, X_test, y_train, y_test, index_train, index_test = sklearn.model_selection.train_test_split(X, y, indices, test_size=test_size)\n",
    "        return {\n",
    "                \"train\": (X_train, y_train, index_train),\n",
    "                \"test\": (X_test, y_test, index_test)\n",
    "               }\n",
    "\n",
    "    @property\n",
    "    def obs(self):\n",
    "        return self._adata.obs\n",
    "    \n",
    "    @property\n",
    "    def var(self):\n",
    "        return self._adata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Put the AnnData object into a Dataset (defined above) object\n",
    "2. Tell the dataset what do we want to predict (the target)\n",
    "3. Check everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "data=Dataset(adata)\n",
    "data.set_target(target)\n",
    "print(data.n_classes)\n",
    "assert data.get_features().shape[0] == len(data.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the encoding. Which class gets which number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Sleep = Code 0\n",
      "Label Wake = Code 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'Sleep', 1: 'Wake'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_encoding([i for i in range(len(data._adata.obs[data._target].unique()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the train / test split\n",
    "\n",
    "The train set is used to modify the weights (parameters) of the model in such a way that the model's prediction is as good as possible\n",
    "\n",
    "But the same data that was used to train the model cannot be used to check how well it does.. very bad models will artificially perform very well when given their training data!\n",
    "\n",
    "This is why we need a test set.\n",
    "\n",
    "**Please note**\n",
    "\n",
    "If we are doing hyperparameter selection based on the data, we also need a validation set to select the right hyperparameters\n",
    "\n",
    "A hyperparameter of the model is everything regarding the model that is not the weights themselves. We could have chosen a different architecture, or a different regularization scheme, learning rate, etc... If we want to find the best one using our dataset, we need to create a separate set\n",
    "\n",
    "For now in this notebook we only need a train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = data.train_test_split(random=random_labels, test_size=0.33) # random split between train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data following some metavariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, metavar: str, test_vals: List):\n",
    "    \"\"\"\n",
    "    Custom split of data based on metavariables\n",
    "    \"\"\"\n",
    "    index_test = np.array([True if r in test_vals else False for r in data._adata.obs[\"Run\"]])\n",
    "    assert np.all(data._adata.obs.loc[index_test][\"Run\"].unique()[0] in test_vals)\n",
    "\n",
    "    index_train = ~index_test\n",
    "\n",
    "    labels  = data.get_labels(random=False)\n",
    "    labels_train = labels[index_train]\n",
    "    labels_test  = labels[index_test]\n",
    "\n",
    "    features  = data.get_features()\n",
    "    features_train = features[index_train, :]\n",
    "    features_test = features[index_test, :]\n",
    "\n",
    "\n",
    "    assert features_train.shape[0] == len(labels_train)\n",
    "    assert features_test.shape[0] == len(labels_test)\n",
    "\n",
    "    print(f\"{features_train.shape[0]} cells in training set\")\n",
    "    print(f\"{features_test.shape[0]} cells in test set\")\n",
    "\n",
    "\n",
    "    dataset_split = {\n",
    "        \"train\": (features_train, labels_train, index_train),\n",
    "        \"test\": (features_test, labels_test, index_test),\n",
    "\n",
    "    }\n",
    "    return dataset_split\n",
    "\n",
    "# dataset_split = train_test_split(data, \"Run\", \"20200916\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define architecture (how many layers, with how many neurons, connectivity between neurons, etc)\n",
    "\n",
    "`units=1000` means create a single hidden layer with 1000 nodes on it. So the MLP model has:\n",
    "\n",
    "* 1 (input) layer with as many nodes as genes N\n",
    "* 1 (hidden) layer where each node has N weights and there are 1000 nodes\n",
    "* 1 (output) layer with 2 nodes (one for each class) where each node has ... 1000 weights because the previous layer has 1000 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sequential_model(units=[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model\n",
    "\n",
    "This sets a loss function and an optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.compile(optimizer='adam', loss=data.get_loss(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 1000)              9996000   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 9,998,002\n",
      "Trainable params: 9,998,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "\n",
    "Populate the nodes with weights that make sense i.e. correctly map the input (genes) to the output (wake/sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2954 samples, validate on 1456 samples\n",
      "Epoch 1/150\n",
      "2954/2954 [==============================] - 1s 310us/sample - loss: 5.9252 - accuracy: 0.5694 - val_loss: 2.6866 - val_accuracy: 0.5144\n",
      "Epoch 2/150\n",
      "2954/2954 [==============================] - 1s 202us/sample - loss: 0.9792 - accuracy: 0.6872 - val_loss: 0.9861 - val_accuracy: 0.6113\n",
      "Epoch 3/150\n",
      "2954/2954 [==============================] - 1s 200us/sample - loss: 0.4161 - accuracy: 0.8311 - val_loss: 0.6862 - val_accuracy: 0.6827\n",
      "Epoch 4/150\n",
      "2954/2954 [==============================] - 1s 211us/sample - loss: 0.2023 - accuracy: 0.9248 - val_loss: 0.6328 - val_accuracy: 0.6957\n",
      "Epoch 5/150\n",
      "2954/2954 [==============================] - 1s 219us/sample - loss: 0.1520 - accuracy: 0.9546 - val_loss: 1.2178 - val_accuracy: 0.6133\n",
      "Epoch 6/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 0.1318 - accuracy: 0.9648 - val_loss: 0.9219 - val_accuracy: 0.6772\n",
      "Epoch 7/150\n",
      "2954/2954 [==============================] - 1s 187us/sample - loss: 0.0755 - accuracy: 0.9885 - val_loss: 1.3367 - val_accuracy: 0.6312\n",
      "Epoch 8/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.1601 - accuracy: 0.9485 - val_loss: 0.9977 - val_accuracy: 0.6745\n",
      "Epoch 9/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 0.0420 - accuracy: 0.9963 - val_loss: 0.7874 - val_accuracy: 0.7205\n",
      "Epoch 10/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0327 - accuracy: 0.9983 - val_loss: 0.7935 - val_accuracy: 0.7122\n",
      "Epoch 11/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.7102\n",
      "Epoch 12/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.8355 - val_accuracy: 0.7115\n",
      "Epoch 13/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.8357 - val_accuracy: 0.7225\n",
      "Epoch 14/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.8522 - val_accuracy: 0.7157\n",
      "Epoch 15/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8635 - val_accuracy: 0.7157\n",
      "Epoch 16/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8860 - val_accuracy: 0.7136\n",
      "Epoch 17/150\n",
      "2954/2954 [==============================] - 1s 179us/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 0.7115\n",
      "Epoch 18/150\n",
      "2954/2954 [==============================] - 1s 180us/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.9026 - val_accuracy: 0.7157\n",
      "Epoch 19/150\n",
      "2954/2954 [==============================] - 1s 213us/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.9213 - val_accuracy: 0.7198\n",
      "Epoch 20/150\n",
      "2954/2954 [==============================] - 1s 207us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9282 - val_accuracy: 0.7184\n",
      "Epoch 21/150\n",
      "2954/2954 [==============================] - 1s 177us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9421 - val_accuracy: 0.7136\n",
      "Epoch 22/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9503 - val_accuracy: 0.7191\n",
      "Epoch 23/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.7198\n",
      "Epoch 24/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9798 - val_accuracy: 0.7191\n",
      "Epoch 25/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9839 - val_accuracy: 0.7212\n",
      "Epoch 26/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9924 - val_accuracy: 0.7212\n",
      "Epoch 27/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 9.1091e-04 - accuracy: 1.0000 - val_loss: 1.0090 - val_accuracy: 0.7184\n",
      "Epoch 28/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 8.1287e-04 - accuracy: 1.0000 - val_loss: 1.0120 - val_accuracy: 0.7198\n",
      "Epoch 29/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 1.5477 - accuracy: 0.7806 - val_loss: 0.6353 - val_accuracy: 0.6690\n",
      "Epoch 30/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 0.3458 - accuracy: 0.8673 - val_loss: 0.9119 - val_accuracy: 0.6126\n",
      "Epoch 31/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 0.2617 - accuracy: 0.9127 - val_loss: 0.6065 - val_accuracy: 0.7115\n",
      "Epoch 32/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 0.1656 - accuracy: 0.9560 - val_loss: 0.7179 - val_accuracy: 0.6971\n",
      "Epoch 33/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 0.1485 - accuracy: 0.9611 - val_loss: 0.7633 - val_accuracy: 0.7047\n",
      "Epoch 34/150\n",
      "2954/2954 [==============================] - 1s 180us/sample - loss: 0.0984 - accuracy: 0.9739 - val_loss: 1.3456 - val_accuracy: 0.6319\n",
      "Epoch 35/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 0.1045 - accuracy: 0.9685 - val_loss: 0.9685 - val_accuracy: 0.7047\n",
      "Epoch 36/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 0.0327 - accuracy: 0.9980 - val_loss: 0.9436 - val_accuracy: 0.7054\n",
      "Epoch 37/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.9379 - val_accuracy: 0.7109\n",
      "Epoch 38/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.9657 - val_accuracy: 0.7109\n",
      "Epoch 39/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.0040 - val_accuracy: 0.7088\n",
      "Epoch 40/150\n",
      "2954/2954 [==============================] - 1s 193us/sample - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.0047 - val_accuracy: 0.7150\n",
      "Epoch 41/150\n",
      "2954/2954 [==============================] - 1s 197us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.0492 - val_accuracy: 0.7060\n",
      "Epoch 42/150\n",
      "2954/2954 [==============================] - 1s 198us/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.0520 - val_accuracy: 0.7129\n",
      "Epoch 43/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.0835 - val_accuracy: 0.7109\n",
      "Epoch 44/150\n",
      "2954/2954 [==============================] - 1s 208us/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.0858 - val_accuracy: 0.7122\n",
      "Epoch 45/150\n",
      "2954/2954 [==============================] - 1s 211us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.0999 - val_accuracy: 0.7095\n",
      "Epoch 46/150\n",
      "2954/2954 [==============================] - 1s 212us/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.1309 - val_accuracy: 0.7081\n",
      "Epoch 47/150\n",
      "2954/2954 [==============================] - 1s 212us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.1633 - val_accuracy: 0.7060\n",
      "Epoch 48/150\n",
      "2954/2954 [==============================] - 1s 214us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.1538 - val_accuracy: 0.7088\n",
      "Epoch 49/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1537 - val_accuracy: 0.7067\n",
      "Epoch 50/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.1722 - val_accuracy: 0.7095\n",
      "Epoch 51/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.1789 - val_accuracy: 0.7081\n",
      "Epoch 52/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.2032 - val_accuracy: 0.7081\n",
      "Epoch 53/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 8.8439e-04 - accuracy: 1.0000 - val_loss: 1.2133 - val_accuracy: 0.7095\n",
      "Epoch 54/150\n",
      "2954/2954 [==============================] - 1s 180us/sample - loss: 7.7847e-04 - accuracy: 1.0000 - val_loss: 1.2433 - val_accuracy: 0.7109\n",
      "Epoch 55/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 6.8960e-04 - accuracy: 1.0000 - val_loss: 1.2409 - val_accuracy: 0.7109\n",
      "Epoch 56/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 5.9868e-04 - accuracy: 1.0000 - val_loss: 1.2546 - val_accuracy: 0.7109\n",
      "Epoch 57/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 5.2641e-04 - accuracy: 1.0000 - val_loss: 1.2601 - val_accuracy: 0.7054\n",
      "Epoch 58/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 4.7302e-04 - accuracy: 1.0000 - val_loss: 1.2789 - val_accuracy: 0.7074\n",
      "Epoch 59/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 4.1674e-04 - accuracy: 1.0000 - val_loss: 1.2740 - val_accuracy: 0.7040\n",
      "Epoch 60/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 3.6725e-04 - accuracy: 1.0000 - val_loss: 1.2842 - val_accuracy: 0.7067\n",
      "Epoch 61/150\n",
      "2954/2954 [==============================] - 1s 191us/sample - loss: 3.3470e-04 - accuracy: 1.0000 - val_loss: 1.2978 - val_accuracy: 0.7047\n",
      "Epoch 62/150\n",
      "2954/2954 [==============================] - 1s 212us/sample - loss: 2.9470e-04 - accuracy: 1.0000 - val_loss: 1.3083 - val_accuracy: 0.7033\n",
      "Epoch 63/150\n",
      "2954/2954 [==============================] - 1s 205us/sample - loss: 2.6458e-04 - accuracy: 1.0000 - val_loss: 1.3242 - val_accuracy: 0.7054\n",
      "Epoch 64/150\n",
      "2954/2954 [==============================] - 1s 207us/sample - loss: 2.4075e-04 - accuracy: 1.0000 - val_loss: 1.3268 - val_accuracy: 0.6999\n",
      "Epoch 65/150\n",
      "2954/2954 [==============================] - 1s 195us/sample - loss: 2.1815e-04 - accuracy: 1.0000 - val_loss: 1.3335 - val_accuracy: 0.7012\n",
      "Epoch 66/150\n",
      "2954/2954 [==============================] - 1s 206us/sample - loss: 1.9767e-04 - accuracy: 1.0000 - val_loss: 1.3485 - val_accuracy: 0.7054\n",
      "Epoch 67/150\n",
      "2954/2954 [==============================] - 1s 212us/sample - loss: 1.7837e-04 - accuracy: 1.0000 - val_loss: 1.3575 - val_accuracy: 0.7054\n",
      "Epoch 68/150\n",
      "2954/2954 [==============================] - 1s 193us/sample - loss: 1.6302e-04 - accuracy: 1.0000 - val_loss: 1.3598 - val_accuracy: 0.7019\n",
      "Epoch 69/150\n",
      "2954/2954 [==============================] - 1s 195us/sample - loss: 1.4728e-04 - accuracy: 1.0000 - val_loss: 1.3786 - val_accuracy: 0.7026\n",
      "Epoch 70/150\n",
      "2954/2954 [==============================] - 1s 188us/sample - loss: 1.3469e-04 - accuracy: 1.0000 - val_loss: 1.3818 - val_accuracy: 0.7012\n",
      "Epoch 71/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 1.2385e-04 - accuracy: 1.0000 - val_loss: 1.3941 - val_accuracy: 0.7054\n",
      "Epoch 72/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 1.1334e-04 - accuracy: 1.0000 - val_loss: 1.3992 - val_accuracy: 0.7019\n",
      "Epoch 73/150\n",
      "2954/2954 [==============================] - 1s 179us/sample - loss: 1.0373e-04 - accuracy: 1.0000 - val_loss: 1.4124 - val_accuracy: 0.7012\n",
      "Epoch 74/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 9.5339e-05 - accuracy: 1.0000 - val_loss: 1.4196 - val_accuracy: 0.7005\n",
      "Epoch 75/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 8.7670e-05 - accuracy: 1.0000 - val_loss: 1.4294 - val_accuracy: 0.7012\n",
      "Epoch 76/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 8.0496e-05 - accuracy: 1.0000 - val_loss: 1.4406 - val_accuracy: 0.7067\n",
      "Epoch 77/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 7.4521e-05 - accuracy: 1.0000 - val_loss: 1.4491 - val_accuracy: 0.7067\n",
      "Epoch 78/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 6.9533e-05 - accuracy: 1.0000 - val_loss: 1.4523 - val_accuracy: 0.7019\n",
      "Epoch 79/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 6.3571e-05 - accuracy: 1.0000 - val_loss: 1.4604 - val_accuracy: 0.7005\n",
      "Epoch 80/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 5.8776e-05 - accuracy: 1.0000 - val_loss: 1.4717 - val_accuracy: 0.7005\n",
      "Epoch 81/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 5.4455e-05 - accuracy: 1.0000 - val_loss: 1.4811 - val_accuracy: 0.7005\n",
      "Epoch 82/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 5.0381e-05 - accuracy: 1.0000 - val_loss: 1.4935 - val_accuracy: 0.7019\n",
      "Epoch 83/150\n",
      "2954/2954 [==============================] - 1s 197us/sample - loss: 4.6567e-05 - accuracy: 1.0000 - val_loss: 1.4954 - val_accuracy: 0.7054\n",
      "Epoch 84/150\n",
      "2954/2954 [==============================] - 1s 210us/sample - loss: 4.3461e-05 - accuracy: 1.0000 - val_loss: 1.5046 - val_accuracy: 0.6999\n",
      "Epoch 85/150\n",
      "2954/2954 [==============================] - 1s 203us/sample - loss: 4.0364e-05 - accuracy: 1.0000 - val_loss: 1.5134 - val_accuracy: 0.6985\n",
      "Epoch 86/150\n",
      "2954/2954 [==============================] - 1s 204us/sample - loss: 3.7262e-05 - accuracy: 1.0000 - val_loss: 1.5234 - val_accuracy: 0.7012\n",
      "Epoch 87/150\n",
      "2954/2954 [==============================] - 1s 208us/sample - loss: 3.4789e-05 - accuracy: 1.0000 - val_loss: 1.5309 - val_accuracy: 0.7005\n",
      "Epoch 88/150\n",
      "2954/2954 [==============================] - 1s 193us/sample - loss: 3.2188e-05 - accuracy: 1.0000 - val_loss: 1.5389 - val_accuracy: 0.7012\n",
      "Epoch 89/150\n",
      "2954/2954 [==============================] - 1s 213us/sample - loss: 3.0163e-05 - accuracy: 1.0000 - val_loss: 1.5439 - val_accuracy: 0.7040\n",
      "Epoch 90/150\n",
      "2954/2954 [==============================] - 1s 207us/sample - loss: 2.7871e-05 - accuracy: 1.0000 - val_loss: 1.5540 - val_accuracy: 0.7019\n",
      "Epoch 91/150\n",
      "2954/2954 [==============================] - 1s 194us/sample - loss: 2.6034e-05 - accuracy: 1.0000 - val_loss: 1.5603 - val_accuracy: 0.6992\n",
      "Epoch 92/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 2.4267e-05 - accuracy: 1.0000 - val_loss: 1.5689 - val_accuracy: 0.7033\n",
      "Epoch 93/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 2.2745e-05 - accuracy: 1.0000 - val_loss: 1.5752 - val_accuracy: 0.7012\n",
      "Epoch 94/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 2.0999e-05 - accuracy: 1.0000 - val_loss: 1.5933 - val_accuracy: 0.7040\n",
      "Epoch 95/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 1.9632e-05 - accuracy: 1.0000 - val_loss: 1.5915 - val_accuracy: 0.7019\n",
      "Epoch 96/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 1.8319e-05 - accuracy: 1.0000 - val_loss: 1.5992 - val_accuracy: 0.7026\n",
      "Epoch 97/150\n",
      "2954/2954 [==============================] - 1s 180us/sample - loss: 1.7195e-05 - accuracy: 1.0000 - val_loss: 1.6114 - val_accuracy: 0.7026\n",
      "Epoch 98/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 1.6002e-05 - accuracy: 1.0000 - val_loss: 1.6154 - val_accuracy: 0.7005\n",
      "Epoch 99/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 1.5001e-05 - accuracy: 1.0000 - val_loss: 1.6263 - val_accuracy: 0.7033\n",
      "Epoch 100/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 1.4039e-05 - accuracy: 1.0000 - val_loss: 1.6298 - val_accuracy: 0.7026\n",
      "Epoch 101/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 1.3107e-05 - accuracy: 1.0000 - val_loss: 1.6342 - val_accuracy: 0.7026\n",
      "Epoch 102/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 1.2298e-05 - accuracy: 1.0000 - val_loss: 1.6424 - val_accuracy: 0.7040\n",
      "Epoch 103/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 1.1525e-05 - accuracy: 1.0000 - val_loss: 1.6522 - val_accuracy: 0.7012\n",
      "Epoch 104/150\n",
      "2954/2954 [==============================] - 1s 213us/sample - loss: 1.0769e-05 - accuracy: 1.0000 - val_loss: 1.6608 - val_accuracy: 0.7019\n",
      "Epoch 105/150\n",
      "2954/2954 [==============================] - 1s 205us/sample - loss: 1.0140e-05 - accuracy: 1.0000 - val_loss: 1.6622 - val_accuracy: 0.7019\n",
      "Epoch 106/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 9.5201e-06 - accuracy: 1.0000 - val_loss: 1.6725 - val_accuracy: 0.7026\n",
      "Epoch 107/150\n",
      "2954/2954 [==============================] - 1s 206us/sample - loss: 8.9133e-06 - accuracy: 1.0000 - val_loss: 1.6812 - val_accuracy: 0.7026\n",
      "Epoch 108/150\n",
      "2954/2954 [==============================] - 1s 204us/sample - loss: 8.3828e-06 - accuracy: 1.0000 - val_loss: 1.6855 - val_accuracy: 0.7033\n",
      "Epoch 109/150\n",
      "2954/2954 [==============================] - 1s 212us/sample - loss: 7.8534e-06 - accuracy: 1.0000 - val_loss: 1.6946 - val_accuracy: 0.7040\n",
      "Epoch 110/150\n",
      "2954/2954 [==============================] - 1s 214us/sample - loss: 7.3917e-06 - accuracy: 1.0000 - val_loss: 1.7007 - val_accuracy: 0.7033\n",
      "Epoch 111/150\n",
      "2954/2954 [==============================] - 1s 201us/sample - loss: 6.9328e-06 - accuracy: 1.0000 - val_loss: 1.7038 - val_accuracy: 0.7019\n",
      "Epoch 112/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 6.5071e-06 - accuracy: 1.0000 - val_loss: 1.7114 - val_accuracy: 0.7033\n",
      "Epoch 113/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 6.1038e-06 - accuracy: 1.0000 - val_loss: 1.7243 - val_accuracy: 0.7033\n",
      "Epoch 114/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 5.7535e-06 - accuracy: 1.0000 - val_loss: 1.7299 - val_accuracy: 0.7040\n",
      "Epoch 115/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 5.3974e-06 - accuracy: 1.0000 - val_loss: 1.7346 - val_accuracy: 0.7047\n",
      "Epoch 116/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 5.0768e-06 - accuracy: 1.0000 - val_loss: 1.7406 - val_accuracy: 0.7047\n",
      "Epoch 117/150\n",
      "2954/2954 [==============================] - 1s 174us/sample - loss: 4.7419e-06 - accuracy: 1.0000 - val_loss: 1.7474 - val_accuracy: 0.7033\n",
      "Epoch 118/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 4.4557e-06 - accuracy: 1.0000 - val_loss: 1.7519 - val_accuracy: 0.7047\n",
      "Epoch 119/150\n",
      "2954/2954 [==============================] - 1s 179us/sample - loss: 4.1549e-06 - accuracy: 1.0000 - val_loss: 1.7603 - val_accuracy: 0.7026\n",
      "Epoch 120/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 3.9134e-06 - accuracy: 1.0000 - val_loss: 1.7673 - val_accuracy: 0.7033\n",
      "Epoch 121/150\n",
      "2954/2954 [==============================] - 1s 181us/sample - loss: 3.6995e-06 - accuracy: 1.0000 - val_loss: 1.7703 - val_accuracy: 0.7047\n",
      "Epoch 122/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 3.4961e-06 - accuracy: 1.0000 - val_loss: 1.7767 - val_accuracy: 0.7047\n",
      "Epoch 123/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 3.2870e-06 - accuracy: 1.0000 - val_loss: 1.7865 - val_accuracy: 0.7054\n",
      "Epoch 124/150\n",
      "2954/2954 [==============================] - 1s 189us/sample - loss: 3.0820e-06 - accuracy: 1.0000 - val_loss: 1.7896 - val_accuracy: 0.7040\n",
      "Epoch 125/150\n",
      "2954/2954 [==============================] - 1s 213us/sample - loss: 2.9075e-06 - accuracy: 1.0000 - val_loss: 1.8007 - val_accuracy: 0.7040\n",
      "Epoch 126/150\n",
      "2954/2954 [==============================] - 1s 214us/sample - loss: 2.7743e-06 - accuracy: 1.0000 - val_loss: 1.8037 - val_accuracy: 0.7060\n",
      "Epoch 127/150\n",
      "2954/2954 [==============================] - 1s 205us/sample - loss: 2.5970e-06 - accuracy: 1.0000 - val_loss: 1.8097 - val_accuracy: 0.7026\n",
      "Epoch 128/150\n",
      "2954/2954 [==============================] - 1s 202us/sample - loss: 2.4556e-06 - accuracy: 1.0000 - val_loss: 1.8160 - val_accuracy: 0.7033\n",
      "Epoch 129/150\n",
      "2954/2954 [==============================] - 1s 198us/sample - loss: 2.3205e-06 - accuracy: 1.0000 - val_loss: 1.8212 - val_accuracy: 0.7060\n",
      "Epoch 130/150\n",
      "2954/2954 [==============================] - 1s 200us/sample - loss: 2.1894e-06 - accuracy: 1.0000 - val_loss: 1.8245 - val_accuracy: 0.7040\n",
      "Epoch 131/150\n",
      "2954/2954 [==============================] - 1s 214us/sample - loss: 2.0671e-06 - accuracy: 1.0000 - val_loss: 1.8367 - val_accuracy: 0.7033\n",
      "Epoch 132/150\n",
      "2954/2954 [==============================] - 1s 206us/sample - loss: 1.9597e-06 - accuracy: 1.0000 - val_loss: 1.8395 - val_accuracy: 0.7060\n",
      "Epoch 133/150\n",
      "2954/2954 [==============================] - 1s 180us/sample - loss: 1.8603e-06 - accuracy: 1.0000 - val_loss: 1.8499 - val_accuracy: 0.7026\n",
      "Epoch 134/150\n",
      "2954/2954 [==============================] - 1s 185us/sample - loss: 1.7524e-06 - accuracy: 1.0000 - val_loss: 1.8506 - val_accuracy: 0.7026\n",
      "Epoch 135/150\n",
      "2954/2954 [==============================] - 1s 179us/sample - loss: 1.6612e-06 - accuracy: 1.0000 - val_loss: 1.8623 - val_accuracy: 0.7019\n",
      "Epoch 136/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 1.5694e-06 - accuracy: 1.0000 - val_loss: 1.8658 - val_accuracy: 0.7026\n",
      "Epoch 137/150\n",
      "2954/2954 [==============================] - 1s 178us/sample - loss: 1.4874e-06 - accuracy: 1.0000 - val_loss: 1.8719 - val_accuracy: 0.7026\n",
      "Epoch 138/150\n",
      "2954/2954 [==============================] - 1s 184us/sample - loss: 1.4143e-06 - accuracy: 1.0000 - val_loss: 1.8801 - val_accuracy: 0.7026\n",
      "Epoch 139/150\n",
      "2954/2954 [==============================] - 1s 187us/sample - loss: 1.3422e-06 - accuracy: 1.0000 - val_loss: 1.8855 - val_accuracy: 0.7026\n",
      "Epoch 140/150\n",
      "2954/2954 [==============================] - 1s 176us/sample - loss: 1.2676e-06 - accuracy: 1.0000 - val_loss: 1.8914 - val_accuracy: 0.7019\n",
      "Epoch 141/150\n",
      "2954/2954 [==============================] - 1s 183us/sample - loss: 1.2012e-06 - accuracy: 1.0000 - val_loss: 1.8960 - val_accuracy: 0.7033\n",
      "Epoch 142/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 1.1366e-06 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 0.7026\n",
      "Epoch 143/150\n",
      "2954/2954 [==============================] - 1s 186us/sample - loss: 1.0751e-06 - accuracy: 1.0000 - val_loss: 1.9141 - val_accuracy: 0.7026\n",
      "Epoch 144/150\n",
      "2954/2954 [==============================] - 1s 182us/sample - loss: 1.0240e-06 - accuracy: 1.0000 - val_loss: 1.9212 - val_accuracy: 0.7026\n",
      "Epoch 145/150\n",
      "2954/2954 [==============================] - 1s 199us/sample - loss: 9.6698e-07 - accuracy: 1.0000 - val_loss: 1.9215 - val_accuracy: 0.7019\n",
      "Epoch 146/150\n",
      "2954/2954 [==============================] - 1s 209us/sample - loss: 9.2598e-07 - accuracy: 1.0000 - val_loss: 1.9299 - val_accuracy: 0.7019\n",
      "Epoch 147/150\n",
      "2954/2954 [==============================] - 1s 206us/sample - loss: 8.7179e-07 - accuracy: 1.0000 - val_loss: 1.9335 - val_accuracy: 0.7026\n",
      "Epoch 148/150\n",
      "2954/2954 [==============================] - 1s 214us/sample - loss: 8.2380e-07 - accuracy: 1.0000 - val_loss: 1.9415 - val_accuracy: 0.7019\n",
      "Epoch 149/150\n",
      "2954/2954 [==============================] - 1s 201us/sample - loss: 7.8293e-07 - accuracy: 1.0000 - val_loss: 1.9431 - val_accuracy: 0.7026\n",
      "Epoch 150/150\n",
      "2954/2954 [==============================] - 1s 209us/sample - loss: 7.3975e-07 - accuracy: 1.0000 - val_loss: 1.9536 - val_accuracy: 0.7040\n"
     ]
    }
   ],
   "source": [
    "history = data.model.fit(*dataset_split[\"train\"][:2], verbose=1, epochs=150, validation_data=dataset_split[\"test\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the history of the accuracy on both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,7)\n",
    "plt.plot(history.history['accuracy'], label=\"train\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"test\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.hlines(y=0.5, xmin=0, xmax=len(history.history[\"accuracy\"]), color=\"red\", linestyles=\"dashed\", label=\"random\")\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.legend(\n",
    "  #['train', 'test'],\n",
    "    loc='upper left')\n",
    "plt.savefig(\"../plots/experiment_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 99% cases training set performance is better than test set performance, because the model is predicting already seen data. That's the case now\n",
    "* The model saturates within the first 20 epochs (the same result would be obtained with just those epochs)\n",
    "* We might be overfitting because the performance is not just very good but perfect on the training set...\n",
    "* More experiments are needed to validate this model, but it's promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "This gives us the accuracy metric of the test set with the model as it is now i.e.\n",
    "the value of the orange line at the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check fraction of cells with class 1 (Wake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.3 % cells are awake and the rest are asleep\n",
      "This number is the accuracy of a random classifier, by chance it will get this fraction right\n"
     ]
    }
   ],
   "source": [
    "print(f\"{round(np.mean(dataset_split['test'][1]), 3) * 100} % cells are awake and the rest are asleep\")\n",
    "# Ideally this is close to 0.5\n",
    "print(\"This number is the accuracy of a random classifier, by chance it will get this fraction right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 70.4 %\n"
     ]
    }
   ],
   "source": [
    "loss, acc = data.model.evaluate(*dataset_split[\"test\"][:2], verbose=0)\n",
    "test_str = 'Test Accuracy: %.1f %%' % (acc * 100)\n",
    "print(test_str)\n",
    "with open(test_str_txt, \"w\") as fh:\n",
    "    fh.write(test_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save(model_name)\n",
    "# data = Dataset.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction for a single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted score: 1.000\n",
      "Predicted class: 0\n",
      "Should be 1\n"
     ]
    }
   ],
   "source": [
    "# pick the first cell of the test set\n",
    "first_cell = dataset_split[\"test\"][0][0,:].tolist()\n",
    "yhat = data.predict([first_cell])\n",
    "print('Predicted score: %.3f' % list(yhat.values())[0])\n",
    "print('Predicted class: %d' % list(yhat.keys())[0])\n",
    "print(f\"Should be {dataset_split['test'][1][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further validation of the result...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibflysleep/anaconda3/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4792ee6c3e784612aa72ec0f6ccbfb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_scores = []\n",
    "class_preds = []\n",
    "\n",
    "test_data = dataset_split[\"test\"][0]\n",
    "\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    yhat = data.predict([test_data[i,:].tolist()])\n",
    "    class_preds.append(list(yhat.keys())[0])\n",
    "    class_scores.append(list(yhat.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = dataset_split[\"test\"][1]\n",
    "\n",
    "summ = pd.DataFrame({\n",
    "    \"yhat\": class_scores,\n",
    "    \"class\": class_preds,\n",
    "    \"ytrue\": ytrue,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>class</th>\n",
       "      <th>ytrue</th>\n",
       "      <th>correct_pred</th>\n",
       "      <th>incorrect_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999989</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851627</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.825658</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0.999993</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0.347401</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0.999823</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1456 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          yhat  class  ytrue  correct_pred  incorrect_pred\n",
       "0     0.999907      0      0          True           False\n",
       "1     1.000000      0      0          True           False\n",
       "2     0.999989      5      5          True           False\n",
       "3     0.851627      2      2          True           False\n",
       "4     0.825658      5      5          True           False\n",
       "...        ...    ...    ...           ...             ...\n",
       "1451  0.999993      2      2          True           False\n",
       "1452  1.000000      1      1          True           False\n",
       "1453  1.000000      6      6          True           False\n",
       "1454  0.347401      1      0         False            True\n",
       "1455  0.999823      2      2          True           False\n",
       "\n",
       "[1456 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[\"correct_pred\"] =  summ.values[:,1] == summ.values[:,2]\n",
    "summ[\"incorrect_pred\"] = summ.values[:,1] != summ.values[:,2]\n",
    "summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_split[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs = pd.merge(dataset_split[\"test\"][2], data.obs, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ=pd.concat([summ, test_obs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>class</th>\n",
       "      <th>ytrue</th>\n",
       "      <th>correct_pred</th>\n",
       "      <th>incorrect_pred</th>\n",
       "      <th>CellID</th>\n",
       "      <th>Age</th>\n",
       "      <th>ClusterID</th>\n",
       "      <th>Clusterings.0</th>\n",
       "      <th>Clusterings.1</th>\n",
       "      <th>...</th>\n",
       "      <th>Sleep_Stage</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>detected</th>\n",
       "      <th>discard</th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>propZero</th>\n",
       "      <th>sizeFactor</th>\n",
       "      <th>sum</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>CAGGGCTCAACAGTGG-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>6-9 days</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 20</td>\n",
       "      <td>Sleep</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1622</td>\n",
       "      <td>4108.0</td>\n",
       "      <td>0.837719</td>\n",
       "      <td>0.674472</td>\n",
       "      <td>4108.0</td>\n",
       "      <td>4108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>TGAGGTTCACTTCAAG-fc9729__DGRP_Mix_Sleep_Depriv...</td>\n",
       "      <td>6-7 days</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>4525.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4525</td>\n",
       "      <td>41944.0</td>\n",
       "      <td>0.547274</td>\n",
       "      <td>7.168933</td>\n",
       "      <td>41944.0</td>\n",
       "      <td>41944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999989</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>GACTCTCGTGTTGACT-ce85ae__S2_10x_160920-20200916</td>\n",
       "      <td>5-7 days</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Wake</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1856</td>\n",
       "      <td>5457.0</td>\n",
       "      <td>0.814307</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>5457.0</td>\n",
       "      <td>5457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851627</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>ATCACTTAGTGAACAT-b0b341__S3_10x_20191023-20191023</td>\n",
       "      <td>7 days</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 20</td>\n",
       "      <td>Sleep</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752</td>\n",
       "      <td>5594.0</td>\n",
       "      <td>0.824712</td>\n",
       "      <td>0.814993</td>\n",
       "      <td>5594.0</td>\n",
       "      <td>5594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.825658</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>AGTCTCCGTCAAATCC-ce85ae__S2_10x_160920-20200916</td>\n",
       "      <td>5-7 days</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>ZT 8</td>\n",
       "      <td>Sleep</td>\n",
       "      <td>658.0</td>\n",
       "      <td>1</td>\n",
       "      <td>658</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>0.934167</td>\n",
       "      <td>0.189393</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>1626.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       yhat  class  ytrue  correct_pred  incorrect_pred  \\\n",
       "0  0.999907      0      0          True           False   \n",
       "1  1.000000      0      0          True           False   \n",
       "2  0.999989      5      5          True           False   \n",
       "3  0.851627      2      2          True           False   \n",
       "4  0.825658      5      5          True           False   \n",
       "\n",
       "                                              CellID       Age ClusterID  \\\n",
       "0  CAGGGCTCAACAGTGG-fc9729__DGRP_Mix_Sleep_Depriv...  6-9 days         4   \n",
       "1  TGAGGTTCACTTCAAG-fc9729__DGRP_Mix_Sleep_Depriv...  6-7 days         4   \n",
       "2    GACTCTCGTGTTGACT-ce85ae__S2_10x_160920-20200916  5-7 days         4   \n",
       "3  ATCACTTAGTGAACAT-b0b341__S3_10x_20191023-20191023    7 days         4   \n",
       "4    AGTCTCCGTCAAATCC-ce85ae__S2_10x_160920-20200916  5-7 days         4   \n",
       "\n",
       "   Clusterings.0  Clusterings.1  ...  Sleep_Stage  Treatment  detected  \\\n",
       "0              3              4  ...        ZT 20      Sleep    1622.0   \n",
       "1              3              4  ...         ZT 8       Wake    4525.0   \n",
       "2              3              4  ...         ZT 8       Wake    1856.0   \n",
       "3              3              4  ...        ZT 20      Sleep    1752.0   \n",
       "4              3              4  ...         ZT 8      Sleep     658.0   \n",
       "\n",
       "   discard  nGene     nUMI  propZero sizeFactor      sum    total  \n",
       "0        0   1622   4108.0  0.837719   0.674472   4108.0   4108.0  \n",
       "1        1   4525  41944.0  0.547274   7.168933  41944.0  41944.0  \n",
       "2        0   1856   5457.0  0.814307   0.895000   5457.0   5457.0  \n",
       "3        0   1752   5594.0  0.824712   0.814993   5594.0   5594.0  \n",
       "4        1    658   1626.0  0.934167   0.189393   1626.0   1626.0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>class</th>\n",
       "      <th>ytrue</th>\n",
       "      <th>correct_pred</th>\n",
       "      <th>incorrect_pred</th>\n",
       "      <th>Clusterings.0</th>\n",
       "      <th>Clusterings.1</th>\n",
       "      <th>Clusterings.2</th>\n",
       "      <th>Clusterings.3</th>\n",
       "      <th>Clusterings.4</th>\n",
       "      <th>...</th>\n",
       "      <th>Embeddings_Y.2</th>\n",
       "      <th>Percent_mito</th>\n",
       "      <th>detected</th>\n",
       "      <th>discard</th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>propZero</th>\n",
       "      <th>sizeFactor</th>\n",
       "      <th>sum</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Condition</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZT 14 SD</th>\n",
       "      <td>31.197010</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>128</td>\n",
       "      <td>96</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>29.591022</td>\n",
       "      <td>3.640718</td>\n",
       "      <td>58805.0</td>\n",
       "      <td>2</td>\n",
       "      <td>58805</td>\n",
       "      <td>205914.0</td>\n",
       "      <td>26.116558</td>\n",
       "      <td>29.317875</td>\n",
       "      <td>205914.0</td>\n",
       "      <td>205914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 14 sleep</th>\n",
       "      <td>77.802381</td>\n",
       "      <td>231</td>\n",
       "      <td>237</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>261</td>\n",
       "      <td>316</td>\n",
       "      <td>237</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>...</td>\n",
       "      <td>34.708839</td>\n",
       "      <td>10.276807</td>\n",
       "      <td>145030.0</td>\n",
       "      <td>9</td>\n",
       "      <td>145030</td>\n",
       "      <td>521367.0</td>\n",
       "      <td>64.489746</td>\n",
       "      <td>74.725090</td>\n",
       "      <td>521367.0</td>\n",
       "      <td>521367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 2 rebound 12h SD</th>\n",
       "      <td>226.487216</td>\n",
       "      <td>443</td>\n",
       "      <td>428</td>\n",
       "      <td>219</td>\n",
       "      <td>10</td>\n",
       "      <td>735</td>\n",
       "      <td>916</td>\n",
       "      <td>687</td>\n",
       "      <td>916</td>\n",
       "      <td>916</td>\n",
       "      <td>...</td>\n",
       "      <td>17.029377</td>\n",
       "      <td>25.054203</td>\n",
       "      <td>436838.0</td>\n",
       "      <td>30</td>\n",
       "      <td>436838</td>\n",
       "      <td>1795278.0</td>\n",
       "      <td>185.294342</td>\n",
       "      <td>255.774628</td>\n",
       "      <td>1795278.0</td>\n",
       "      <td>1795278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 2 sleep drive 14h SD</th>\n",
       "      <td>144.128526</td>\n",
       "      <td>229</td>\n",
       "      <td>226</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>459</td>\n",
       "      <td>580</td>\n",
       "      <td>435</td>\n",
       "      <td>580</td>\n",
       "      <td>580</td>\n",
       "      <td>...</td>\n",
       "      <td>8.989573</td>\n",
       "      <td>15.420468</td>\n",
       "      <td>267025.0</td>\n",
       "      <td>18</td>\n",
       "      <td>267025</td>\n",
       "      <td>1004200.0</td>\n",
       "      <td>118.284142</td>\n",
       "      <td>141.358109</td>\n",
       "      <td>1004200.0</td>\n",
       "      <td>1004200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 20 midline cross</th>\n",
       "      <td>88.244600</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>283</td>\n",
       "      <td>356</td>\n",
       "      <td>267</td>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "      <td>...</td>\n",
       "      <td>22.005627</td>\n",
       "      <td>6.570885</td>\n",
       "      <td>175916.0</td>\n",
       "      <td>12</td>\n",
       "      <td>175916</td>\n",
       "      <td>673128.0</td>\n",
       "      <td>71.399597</td>\n",
       "      <td>96.645912</td>\n",
       "      <td>673128.0</td>\n",
       "      <td>673128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 20 sleep</th>\n",
       "      <td>287.025583</td>\n",
       "      <td>548</td>\n",
       "      <td>528</td>\n",
       "      <td>262</td>\n",
       "      <td>36</td>\n",
       "      <td>926</td>\n",
       "      <td>1192</td>\n",
       "      <td>894</td>\n",
       "      <td>1192</td>\n",
       "      <td>1192</td>\n",
       "      <td>...</td>\n",
       "      <td>100.646004</td>\n",
       "      <td>24.244793</td>\n",
       "      <td>548298.0</td>\n",
       "      <td>49</td>\n",
       "      <td>548298</td>\n",
       "      <td>2002588.0</td>\n",
       "      <td>243.142776</td>\n",
       "      <td>301.424683</td>\n",
       "      <td>2002588.0</td>\n",
       "      <td>2002588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 20 sleep deprivation</th>\n",
       "      <td>215.223485</td>\n",
       "      <td>401</td>\n",
       "      <td>369</td>\n",
       "      <td>194</td>\n",
       "      <td>30</td>\n",
       "      <td>696</td>\n",
       "      <td>896</td>\n",
       "      <td>672</td>\n",
       "      <td>896</td>\n",
       "      <td>896</td>\n",
       "      <td>...</td>\n",
       "      <td>71.477463</td>\n",
       "      <td>21.231804</td>\n",
       "      <td>411345.0</td>\n",
       "      <td>36</td>\n",
       "      <td>411345</td>\n",
       "      <td>1459433.0</td>\n",
       "      <td>182.844925</td>\n",
       "      <td>219.431213</td>\n",
       "      <td>1459433.0</td>\n",
       "      <td>1459433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 8 20h SD</th>\n",
       "      <td>63.190920</td>\n",
       "      <td>342</td>\n",
       "      <td>362</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>207</td>\n",
       "      <td>276</td>\n",
       "      <td>207</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>...</td>\n",
       "      <td>40.386845</td>\n",
       "      <td>6.251998</td>\n",
       "      <td>114716.0</td>\n",
       "      <td>20</td>\n",
       "      <td>114716</td>\n",
       "      <td>395232.0</td>\n",
       "      <td>57.522663</td>\n",
       "      <td>62.329391</td>\n",
       "      <td>395232.0</td>\n",
       "      <td>395232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 8 gab</th>\n",
       "      <td>76.115061</td>\n",
       "      <td>420</td>\n",
       "      <td>438</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>251</td>\n",
       "      <td>324</td>\n",
       "      <td>243</td>\n",
       "      <td>324</td>\n",
       "      <td>324</td>\n",
       "      <td>...</td>\n",
       "      <td>72.935493</td>\n",
       "      <td>7.518210</td>\n",
       "      <td>129407.0</td>\n",
       "      <td>21</td>\n",
       "      <td>129407</td>\n",
       "      <td>400524.0</td>\n",
       "      <td>68.052826</td>\n",
       "      <td>63.522930</td>\n",
       "      <td>400524.0</td>\n",
       "      <td>400524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 8 wake</th>\n",
       "      <td>158.688815</td>\n",
       "      <td>423</td>\n",
       "      <td>411</td>\n",
       "      <td>155</td>\n",
       "      <td>7</td>\n",
       "      <td>486</td>\n",
       "      <td>648</td>\n",
       "      <td>486</td>\n",
       "      <td>648</td>\n",
       "      <td>648</td>\n",
       "      <td>...</td>\n",
       "      <td>142.173859</td>\n",
       "      <td>9.226872</td>\n",
       "      <td>287857.0</td>\n",
       "      <td>15</td>\n",
       "      <td>287857</td>\n",
       "      <td>923083.0</td>\n",
       "      <td>133.199905</td>\n",
       "      <td>150.204742</td>\n",
       "      <td>923083.0</td>\n",
       "      <td>923083.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZT 8 wake stimulation</th>\n",
       "      <td>47.570649</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>192</td>\n",
       "      <td>144</td>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>37.510910</td>\n",
       "      <td>1.725349</td>\n",
       "      <td>107851.0</td>\n",
       "      <td>16</td>\n",
       "      <td>107851</td>\n",
       "      <td>469028.0</td>\n",
       "      <td>37.209503</td>\n",
       "      <td>79.552223</td>\n",
       "      <td>469028.0</td>\n",
       "      <td>469028.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               yhat  class  ytrue  correct_pred  \\\n",
       "Condition                                                         \n",
       "ZT 14 SD                  31.197010     94     96            31   \n",
       "ZT 14 sleep               77.802381    231    237            76   \n",
       "ZT 2 rebound 12h SD      226.487216    443    428           219   \n",
       "ZT 2 sleep drive 14h SD  144.128526    229    226           140   \n",
       "ZT 20 midline cross       88.244600    177    178            88   \n",
       "ZT 20 sleep              287.025583    548    528           262   \n",
       "ZT 20 sleep deprivation  215.223485    401    369           194   \n",
       "ZT 8 20h SD               63.190920    342    362            55   \n",
       "ZT 8 gab                  76.115061    420    438            71   \n",
       "ZT 8 wake                158.688815    423    411           155   \n",
       "ZT 8 wake stimulation     47.570649      4      0            47   \n",
       "\n",
       "                         incorrect_pred  Clusterings.0  Clusterings.1  \\\n",
       "Condition                                                               \n",
       "ZT 14 SD                              1             96            128   \n",
       "ZT 14 sleep                           3            261            316   \n",
       "ZT 2 rebound 12h SD                  10            735            916   \n",
       "ZT 2 sleep drive 14h SD               5            459            580   \n",
       "ZT 20 midline cross                   1            283            356   \n",
       "ZT 20 sleep                          36            926           1192   \n",
       "ZT 20 sleep deprivation              30            696            896   \n",
       "ZT 8 20h SD                          14            207            276   \n",
       "ZT 8 gab                             10            251            324   \n",
       "ZT 8 wake                             7            486            648   \n",
       "ZT 8 wake stimulation                 1            160            192   \n",
       "\n",
       "                         Clusterings.2  Clusterings.3  Clusterings.4  ...  \\\n",
       "Condition                                                             ...   \n",
       "ZT 14 SD                            96            128            128  ...   \n",
       "ZT 14 sleep                        237            316            316  ...   \n",
       "ZT 2 rebound 12h SD                687            916            916  ...   \n",
       "ZT 2 sleep drive 14h SD            435            580            580  ...   \n",
       "ZT 20 midline cross                267            356            356  ...   \n",
       "ZT 20 sleep                        894           1192           1192  ...   \n",
       "ZT 20 sleep deprivation            672            896            896  ...   \n",
       "ZT 8 20h SD                        207            276            276  ...   \n",
       "ZT 8 gab                           243            324            324  ...   \n",
       "ZT 8 wake                          486            648            648  ...   \n",
       "ZT 8 wake stimulation              144            192            192  ...   \n",
       "\n",
       "                         Embeddings_Y.2  Percent_mito  detected  discard  \\\n",
       "Condition                                                                  \n",
       "ZT 14 SD                      29.591022      3.640718   58805.0        2   \n",
       "ZT 14 sleep                   34.708839     10.276807  145030.0        9   \n",
       "ZT 2 rebound 12h SD           17.029377     25.054203  436838.0       30   \n",
       "ZT 2 sleep drive 14h SD        8.989573     15.420468  267025.0       18   \n",
       "ZT 20 midline cross           22.005627      6.570885  175916.0       12   \n",
       "ZT 20 sleep                  100.646004     24.244793  548298.0       49   \n",
       "ZT 20 sleep deprivation       71.477463     21.231804  411345.0       36   \n",
       "ZT 8 20h SD                   40.386845      6.251998  114716.0       20   \n",
       "ZT 8 gab                      72.935493      7.518210  129407.0       21   \n",
       "ZT 8 wake                    142.173859      9.226872  287857.0       15   \n",
       "ZT 8 wake stimulation         37.510910      1.725349  107851.0       16   \n",
       "\n",
       "                          nGene       nUMI    propZero  sizeFactor        sum  \\\n",
       "Condition                                                                       \n",
       "ZT 14 SD                  58805   205914.0   26.116558   29.317875   205914.0   \n",
       "ZT 14 sleep              145030   521367.0   64.489746   74.725090   521367.0   \n",
       "ZT 2 rebound 12h SD      436838  1795278.0  185.294342  255.774628  1795278.0   \n",
       "ZT 2 sleep drive 14h SD  267025  1004200.0  118.284142  141.358109  1004200.0   \n",
       "ZT 20 midline cross      175916   673128.0   71.399597   96.645912   673128.0   \n",
       "ZT 20 sleep              548298  2002588.0  243.142776  301.424683  2002588.0   \n",
       "ZT 20 sleep deprivation  411345  1459433.0  182.844925  219.431213  1459433.0   \n",
       "ZT 8 20h SD              114716   395232.0   57.522663   62.329391   395232.0   \n",
       "ZT 8 gab                 129407   400524.0   68.052826   63.522930   400524.0   \n",
       "ZT 8 wake                287857   923083.0  133.199905  150.204742   923083.0   \n",
       "ZT 8 wake stimulation    107851   469028.0   37.209503   79.552223   469028.0   \n",
       "\n",
       "                             total  \n",
       "Condition                           \n",
       "ZT 14 SD                  205914.0  \n",
       "ZT 14 sleep               521367.0  \n",
       "ZT 2 rebound 12h SD      1795278.0  \n",
       "ZT 2 sleep drive 14h SD  1004200.0  \n",
       "ZT 20 midline cross       673128.0  \n",
       "ZT 20 sleep              2002588.0  \n",
       "ZT 20 sleep deprivation  1459433.0  \n",
       "ZT 8 20h SD               395232.0  \n",
       "ZT 8 gab                  400524.0  \n",
       "ZT 8 wake                 923083.0  \n",
       "ZT 8 wake stimulation     469028.0  \n",
       "\n",
       "[11 rows x 29 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ.groupby(\"Condition\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>class</th>\n",
       "      <th>ytrue</th>\n",
       "      <th>correct_pred</th>\n",
       "      <th>incorrect_pred</th>\n",
       "      <th>Clusterings.0</th>\n",
       "      <th>Clusterings.1</th>\n",
       "      <th>Clusterings.2</th>\n",
       "      <th>Clusterings.3</th>\n",
       "      <th>Clusterings.4</th>\n",
       "      <th>...</th>\n",
       "      <th>Embeddings_Y.2</th>\n",
       "      <th>Percent_mito</th>\n",
       "      <th>detected</th>\n",
       "      <th>discard</th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>propZero</th>\n",
       "      <th>sizeFactor</th>\n",
       "      <th>sum</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genotype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line_287</th>\n",
       "      <td>155.409705</td>\n",
       "      <td>579</td>\n",
       "      <td>573</td>\n",
       "      <td>144</td>\n",
       "      <td>16</td>\n",
       "      <td>488</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>107.590340</td>\n",
       "      <td>13.756172</td>\n",
       "      <td>260098.0</td>\n",
       "      <td>15</td>\n",
       "      <td>260098</td>\n",
       "      <td>798320.0</td>\n",
       "      <td>133.977188</td>\n",
       "      <td>119.779808</td>\n",
       "      <td>798320.0</td>\n",
       "      <td>798320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_303</th>\n",
       "      <td>162.372277</td>\n",
       "      <td>396</td>\n",
       "      <td>397</td>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>522</td>\n",
       "      <td>664</td>\n",
       "      <td>498</td>\n",
       "      <td>664</td>\n",
       "      <td>664</td>\n",
       "      <td>...</td>\n",
       "      <td>44.577148</td>\n",
       "      <td>15.555780</td>\n",
       "      <td>320399.0</td>\n",
       "      <td>27</td>\n",
       "      <td>320399</td>\n",
       "      <td>1309891.0</td>\n",
       "      <td>133.944077</td>\n",
       "      <td>192.866348</td>\n",
       "      <td>1309891.0</td>\n",
       "      <td>1309891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_313</th>\n",
       "      <td>68.884057</td>\n",
       "      <td>68</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>280</td>\n",
       "      <td>210</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>24.431211</td>\n",
       "      <td>3.838529</td>\n",
       "      <td>151599.0</td>\n",
       "      <td>15</td>\n",
       "      <td>151599</td>\n",
       "      <td>621403.0</td>\n",
       "      <td>54.832516</td>\n",
       "      <td>98.771049</td>\n",
       "      <td>621403.0</td>\n",
       "      <td>621403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_359</th>\n",
       "      <td>15.751792</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>17.128075</td>\n",
       "      <td>0.794291</td>\n",
       "      <td>31909.0</td>\n",
       "      <td>4</td>\n",
       "      <td>31909</td>\n",
       "      <td>124335.0</td>\n",
       "      <td>12.807504</td>\n",
       "      <td>20.968523</td>\n",
       "      <td>124335.0</td>\n",
       "      <td>124335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_379</th>\n",
       "      <td>259.685716</td>\n",
       "      <td>655</td>\n",
       "      <td>656</td>\n",
       "      <td>244</td>\n",
       "      <td>26</td>\n",
       "      <td>850</td>\n",
       "      <td>1080</td>\n",
       "      <td>810</td>\n",
       "      <td>1080</td>\n",
       "      <td>1080</td>\n",
       "      <td>...</td>\n",
       "      <td>87.123665</td>\n",
       "      <td>26.131338</td>\n",
       "      <td>503289.0</td>\n",
       "      <td>50</td>\n",
       "      <td>503289</td>\n",
       "      <td>1864700.0</td>\n",
       "      <td>219.645920</td>\n",
       "      <td>280.476501</td>\n",
       "      <td>1864700.0</td>\n",
       "      <td>1864700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_441</th>\n",
       "      <td>242.670214</td>\n",
       "      <td>524</td>\n",
       "      <td>515</td>\n",
       "      <td>229</td>\n",
       "      <td>19</td>\n",
       "      <td>776</td>\n",
       "      <td>992</td>\n",
       "      <td>744</td>\n",
       "      <td>992</td>\n",
       "      <td>992</td>\n",
       "      <td>...</td>\n",
       "      <td>110.844986</td>\n",
       "      <td>20.976332</td>\n",
       "      <td>446411.0</td>\n",
       "      <td>39</td>\n",
       "      <td>446411</td>\n",
       "      <td>1567675.0</td>\n",
       "      <td>203.336563</td>\n",
       "      <td>234.721939</td>\n",
       "      <td>1567675.0</td>\n",
       "      <td>1567675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_646</th>\n",
       "      <td>109.026463</td>\n",
       "      <td>353</td>\n",
       "      <td>351</td>\n",
       "      <td>106</td>\n",
       "      <td>7</td>\n",
       "      <td>387</td>\n",
       "      <td>452</td>\n",
       "      <td>339</td>\n",
       "      <td>452</td>\n",
       "      <td>452</td>\n",
       "      <td>...</td>\n",
       "      <td>38.902752</td>\n",
       "      <td>11.826299</td>\n",
       "      <td>203800.0</td>\n",
       "      <td>20</td>\n",
       "      <td>203800</td>\n",
       "      <td>771933.0</td>\n",
       "      <td>92.609802</td>\n",
       "      <td>115.383301</td>\n",
       "      <td>771933.0</td>\n",
       "      <td>771933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_88</th>\n",
       "      <td>247.688806</td>\n",
       "      <td>320</td>\n",
       "      <td>306</td>\n",
       "      <td>239</td>\n",
       "      <td>14</td>\n",
       "      <td>767</td>\n",
       "      <td>1012</td>\n",
       "      <td>759</td>\n",
       "      <td>1012</td>\n",
       "      <td>1012</td>\n",
       "      <td>...</td>\n",
       "      <td>104.420280</td>\n",
       "      <td>22.582960</td>\n",
       "      <td>468627.0</td>\n",
       "      <td>29</td>\n",
       "      <td>468627</td>\n",
       "      <td>1627937.0</td>\n",
       "      <td>206.113861</td>\n",
       "      <td>245.407028</td>\n",
       "      <td>1627937.0</td>\n",
       "      <td>1627937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_892</th>\n",
       "      <td>63.732961</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>256</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.215744</td>\n",
       "      <td>6.604895</td>\n",
       "      <td>125218.0</td>\n",
       "      <td>9</td>\n",
       "      <td>125218</td>\n",
       "      <td>527045.0</td>\n",
       "      <td>51.471935</td>\n",
       "      <td>72.548286</td>\n",
       "      <td>527045.0</td>\n",
       "      <td>527045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_908</th>\n",
       "      <td>90.452254</td>\n",
       "      <td>313</td>\n",
       "      <td>307</td>\n",
       "      <td>83</td>\n",
       "      <td>13</td>\n",
       "      <td>288</td>\n",
       "      <td>384</td>\n",
       "      <td>288</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>...</td>\n",
       "      <td>51.652294</td>\n",
       "      <td>9.095512</td>\n",
       "      <td>171738.0</td>\n",
       "      <td>20</td>\n",
       "      <td>171738</td>\n",
       "      <td>636536.0</td>\n",
       "      <td>78.817612</td>\n",
       "      <td>93.364006</td>\n",
       "      <td>636536.0</td>\n",
       "      <td>636536.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                yhat  class  ytrue  correct_pred  incorrect_pred  \\\n",
       "Genotype                                                           \n",
       "line_287  155.409705    579    573           144              16   \n",
       "line_303  162.372277    396    397           146              20   \n",
       "line_313   68.884057     68     67            69               1   \n",
       "line_359   15.751792      4      0            15               1   \n",
       "line_379  259.685716    655    656           244              26   \n",
       "line_441  242.670214    524    515           229              19   \n",
       "line_646  109.026463    353    351           106               7   \n",
       "line_88   247.688806    320    306           239              14   \n",
       "line_892   63.732961    100    101            63               1   \n",
       "line_908   90.452254    313    307            83              13   \n",
       "\n",
       "          Clusterings.0  Clusterings.1  Clusterings.2  Clusterings.3  \\\n",
       "Genotype                                                               \n",
       "line_287            488            640            480            640   \n",
       "line_303            522            664            498            664   \n",
       "line_313            234            280            210            280   \n",
       "line_359             48             64             48             64   \n",
       "line_379            850           1080            810           1080   \n",
       "line_441            776            992            744            992   \n",
       "line_646            387            452            339            452   \n",
       "line_88             767           1012            759           1012   \n",
       "line_892            200            256            192            256   \n",
       "line_908            288            384            288            384   \n",
       "\n",
       "          Clusterings.4  ...  Embeddings_Y.2  Percent_mito  detected  discard  \\\n",
       "Genotype                 ...                                                    \n",
       "line_287            640  ...      107.590340     13.756172  260098.0       15   \n",
       "line_303            664  ...       44.577148     15.555780  320399.0       27   \n",
       "line_313            280  ...       24.431211      3.838529  151599.0       15   \n",
       "line_359             64  ...       17.128075      0.794291   31909.0        4   \n",
       "line_379           1080  ...       87.123665     26.131338  503289.0       50   \n",
       "line_441            992  ...      110.844986     20.976332  446411.0       39   \n",
       "line_646            452  ...       38.902752     11.826299  203800.0       20   \n",
       "line_88            1012  ...      104.420280     22.582960  468627.0       29   \n",
       "line_892            256  ...       -9.215744      6.604895  125218.0        9   \n",
       "line_908            384  ...       51.652294      9.095512  171738.0       20   \n",
       "\n",
       "           nGene       nUMI    propZero  sizeFactor        sum      total  \n",
       "Genotype                                                                   \n",
       "line_287  260098   798320.0  133.977188  119.779808   798320.0   798320.0  \n",
       "line_303  320399  1309891.0  133.944077  192.866348  1309891.0  1309891.0  \n",
       "line_313  151599   621403.0   54.832516   98.771049   621403.0   621403.0  \n",
       "line_359   31909   124335.0   12.807504   20.968523   124335.0   124335.0  \n",
       "line_379  503289  1864700.0  219.645920  280.476501  1864700.0  1864700.0  \n",
       "line_441  446411  1567675.0  203.336563  234.721939  1567675.0  1567675.0  \n",
       "line_646  203800   771933.0   92.609802  115.383301   771933.0   771933.0  \n",
       "line_88   468627  1627937.0  206.113861  245.407028  1627937.0  1627937.0  \n",
       "line_892  125218   527045.0   51.471935   72.548286   527045.0   527045.0  \n",
       "line_908  171738   636536.0   78.817612   93.364006   636536.0   636536.0  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ.groupby(\"Genotype\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>class</th>\n",
       "      <th>ytrue</th>\n",
       "      <th>correct_pred</th>\n",
       "      <th>incorrect_pred</th>\n",
       "      <th>Clusterings.0</th>\n",
       "      <th>Clusterings.1</th>\n",
       "      <th>Clusterings.2</th>\n",
       "      <th>Clusterings.3</th>\n",
       "      <th>Clusterings.4</th>\n",
       "      <th>...</th>\n",
       "      <th>Embeddings_Y.2</th>\n",
       "      <th>Percent_mito</th>\n",
       "      <th>detected</th>\n",
       "      <th>discard</th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>propZero</th>\n",
       "      <th>sizeFactor</th>\n",
       "      <th>sum</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20180419</th>\n",
       "      <td>234.614130</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>8</td>\n",
       "      <td>760</td>\n",
       "      <td>960</td>\n",
       "      <td>720</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>...</td>\n",
       "      <td>174.983536</td>\n",
       "      <td>10.607087</td>\n",
       "      <td>461040.0</td>\n",
       "      <td>51</td>\n",
       "      <td>461040</td>\n",
       "      <td>1645433.0</td>\n",
       "      <td>193.872940</td>\n",
       "      <td>273.382324</td>\n",
       "      <td>1645433.0</td>\n",
       "      <td>1645433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191009</th>\n",
       "      <td>337.009635</td>\n",
       "      <td>370</td>\n",
       "      <td>342</td>\n",
       "      <td>329</td>\n",
       "      <td>13</td>\n",
       "      <td>1066</td>\n",
       "      <td>1368</td>\n",
       "      <td>1026</td>\n",
       "      <td>1368</td>\n",
       "      <td>1368</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.366936</td>\n",
       "      <td>45.411392</td>\n",
       "      <td>624681.0</td>\n",
       "      <td>37</td>\n",
       "      <td>624681</td>\n",
       "      <td>2294930.0</td>\n",
       "      <td>279.500641</td>\n",
       "      <td>323.791565</td>\n",
       "      <td>2294930.0</td>\n",
       "      <td>2294930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191023</th>\n",
       "      <td>347.025318</td>\n",
       "      <td>696</td>\n",
       "      <td>698</td>\n",
       "      <td>340</td>\n",
       "      <td>9</td>\n",
       "      <td>1119</td>\n",
       "      <td>1396</td>\n",
       "      <td>1047</td>\n",
       "      <td>1396</td>\n",
       "      <td>1396</td>\n",
       "      <td>...</td>\n",
       "      <td>74.231094</td>\n",
       "      <td>27.350485</td>\n",
       "      <td>693884.0</td>\n",
       "      <td>50</td>\n",
       "      <td>693884</td>\n",
       "      <td>2869214.0</td>\n",
       "      <td>279.576874</td>\n",
       "      <td>411.135834</td>\n",
       "      <td>2869214.0</td>\n",
       "      <td>2869214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20191106</th>\n",
       "      <td>155.878722</td>\n",
       "      <td>466</td>\n",
       "      <td>474</td>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>498</td>\n",
       "      <td>632</td>\n",
       "      <td>474</td>\n",
       "      <td>632</td>\n",
       "      <td>632</td>\n",
       "      <td>...</td>\n",
       "      <td>82.272377</td>\n",
       "      <td>19.257898</td>\n",
       "      <td>284178.0</td>\n",
       "      <td>14</td>\n",
       "      <td>284178</td>\n",
       "      <td>998975.0</td>\n",
       "      <td>129.567978</td>\n",
       "      <td>142.707123</td>\n",
       "      <td>998975.0</td>\n",
       "      <td>998975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200902</th>\n",
       "      <td>123.954674</td>\n",
       "      <td>578</td>\n",
       "      <td>548</td>\n",
       "      <td>81</td>\n",
       "      <td>56</td>\n",
       "      <td>419</td>\n",
       "      <td>548</td>\n",
       "      <td>411</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>...</td>\n",
       "      <td>106.674614</td>\n",
       "      <td>9.142261</td>\n",
       "      <td>239939.0</td>\n",
       "      <td>31</td>\n",
       "      <td>239939</td>\n",
       "      <td>825073.0</td>\n",
       "      <td>112.994095</td>\n",
       "      <td>130.569183</td>\n",
       "      <td>825073.0</td>\n",
       "      <td>825073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200916</th>\n",
       "      <td>158.344620</td>\n",
       "      <td>817</td>\n",
       "      <td>845</td>\n",
       "      <td>147</td>\n",
       "      <td>22</td>\n",
       "      <td>515</td>\n",
       "      <td>676</td>\n",
       "      <td>507</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>...</td>\n",
       "      <td>138.321213</td>\n",
       "      <td>14.019900</td>\n",
       "      <td>281692.0</td>\n",
       "      <td>23</td>\n",
       "      <td>281692</td>\n",
       "      <td>897814.0</td>\n",
       "      <td>140.816711</td>\n",
       "      <td>140.956940</td>\n",
       "      <td>897814.0</td>\n",
       "      <td>897814.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20200924</th>\n",
       "      <td>58.847145</td>\n",
       "      <td>354</td>\n",
       "      <td>366</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>183</td>\n",
       "      <td>244</td>\n",
       "      <td>183</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>49.339127</td>\n",
       "      <td>5.373086</td>\n",
       "      <td>97674.0</td>\n",
       "      <td>22</td>\n",
       "      <td>97674</td>\n",
       "      <td>318336.0</td>\n",
       "      <td>51.227715</td>\n",
       "      <td>51.743824</td>\n",
       "      <td>318336.0</td>\n",
       "      <td>318336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                yhat  class  ytrue  correct_pred  incorrect_pred  \\\n",
       "Run                                                                \n",
       "20180419  234.614130     31      0           232               8   \n",
       "20191009  337.009635    370    342           329              13   \n",
       "20191023  347.025318    696    698           340               9   \n",
       "20191106  155.878722    466    474           154               4   \n",
       "20200902  123.954674    578    548            81              56   \n",
       "20200916  158.344620    817    845           147              22   \n",
       "20200924   58.847145    354    366            55               6   \n",
       "\n",
       "          Clusterings.0  Clusterings.1  Clusterings.2  Clusterings.3  \\\n",
       "Run                                                                    \n",
       "20180419            760            960            720            960   \n",
       "20191009           1066           1368           1026           1368   \n",
       "20191023           1119           1396           1047           1396   \n",
       "20191106            498            632            474            632   \n",
       "20200902            419            548            411            548   \n",
       "20200916            515            676            507            676   \n",
       "20200924            183            244            183            244   \n",
       "\n",
       "          Clusterings.4  ...  Embeddings_Y.2  Percent_mito  detected  discard  \\\n",
       "Run                      ...                                                    \n",
       "20180419            960  ...      174.983536     10.607087  461040.0       51   \n",
       "20191009           1368  ...      -48.366936     45.411392  624681.0       37   \n",
       "20191023           1396  ...       74.231094     27.350485  693884.0       50   \n",
       "20191106            632  ...       82.272377     19.257898  284178.0       14   \n",
       "20200902            548  ...      106.674614      9.142261  239939.0       31   \n",
       "20200916            676  ...      138.321213     14.019900  281692.0       23   \n",
       "20200924            244  ...       49.339127      5.373086   97674.0       22   \n",
       "\n",
       "           nGene       nUMI    propZero  sizeFactor        sum      total  \n",
       "Run                                                                        \n",
       "20180419  461040  1645433.0  193.872940  273.382324  1645433.0  1645433.0  \n",
       "20191009  624681  2294930.0  279.500641  323.791565  2294930.0  2294930.0  \n",
       "20191023  693884  2869214.0  279.576874  411.135834  2869214.0  2869214.0  \n",
       "20191106  284178   998975.0  129.567978  142.707123   998975.0   998975.0  \n",
       "20200902  239939   825073.0  112.994095  130.569183   825073.0   825073.0  \n",
       "20200916  281692   897814.0  140.816711  140.956940   897814.0   897814.0  \n",
       "20200924   97674   318336.0   51.227715   51.743824   318336.0   318336.0  \n",
       "\n",
       "[7 rows x 29 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ.groupby(\"Run\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ.to_csv(summary_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZT 20 sleep                271\n",
       "ZT 20 sleep deprivation    230\n",
       "ZT 2 rebound 12h SD        208\n",
       "ZT 2 sleep drive 14h SD    167\n",
       "ZT 8 wake                  155\n",
       "ZT 14 sleep                 90\n",
       "ZT 20 midline cross         87\n",
       "ZT 8 gab                    82\n",
       "ZT 8 20h SD                 81\n",
       "ZT 14 SD                    45\n",
       "ZT 8 wake stimulation       40\n",
       "Name: Condition, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summ[\"Condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
